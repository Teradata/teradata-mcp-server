#!/usr/bin/env python3
"""Simple MCP Performance Test Runner"""

import asyncio
import json
import logging
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List
from mcp_streamable_client import MCPStreamableClient


def load_config(config_file: str) -> Dict:
    with open(config_file, 'r') as f:
        return json.load(f)


async def run_test(config_file: str, verbose: bool = False):
    config = load_config(config_file)

    # Setup logging with verbose flag
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    server = config['server']
    server_url = f"http://{server['host']}:{server['port']}"

    print(f"\n{'='*60}")
    print(f"MCP PERFORMANCE TEST")
    print(f"Server: {server_url}")
    print(f"Streams: {len(config['streams'])}")
    print(f"{'='*60}\n")

    test_start_time = time.time()

    # Create and run clients
    clients = []
    for stream_config in config['streams']:
        client = MCPStreamableClient(
            stream_id=stream_config.get('stream_id', 'test'),
            server_url=server_url,
            test_config_path=stream_config['test_config'],
            duration_seconds=stream_config.get('duration', 10),
            loop_tests=stream_config.get('loop', False),
            auth=stream_config.get('auth')
        )
        clients.append(client)

    # Run all clients with staggered starts to avoid initialization conflicts
    tasks = []
    for i, client in enumerate(clients):
        # Stagger starts by 0.5 seconds
        async def run_with_delay(c, delay):
            await asyncio.sleep(delay)
            return await c.run()

        tasks.append(run_with_delay(client, i * 0.5))

    await asyncio.gather(*tasks)
    test_end_time = time.time()

    # Collect all metrics
    all_metrics = []
    total_requests = 0
    total_successful = 0
    total_failed = 0
    total_duration = test_end_time - test_start_time

    for client in clients:
        metrics = client.get_metrics()
        all_metrics.append(metrics)
        total_requests += metrics['total_requests']
        total_successful += metrics['successful_requests']
        total_failed += metrics['failed_requests']

    # Display results
    print(f"\n{'='*60}")
    print(f"RESULTS")
    print(f"{'='*60}")

    for metrics in all_metrics:
        print(f"\nStream {metrics['stream_id']}:")
        print(f"  Requests: {metrics['total_requests']}")
        print(f"  Success Rate: {metrics['success_rate']:.1f}%")
        print(f"  Avg Response: {metrics['avg_response_time']*1000:.2f}ms")
        print(f"  Throughput: {metrics['requests_per_second']:.2f} req/s")

    print(f"\nOVERALL:")
    print(f"  Total Requests: {total_requests}")
    print(f"  Successful: {total_successful}")
    print(f"  Failed: {total_failed}")
    if total_requests > 0:
        print(f"  Success Rate: {(total_successful/total_requests*100):.1f}%")
        print(f"  Overall Throughput: {total_requests/total_duration:.2f} req/s")
    print(f"{'='*60}\n")

    # Generate detailed report
    generate_report(config, all_metrics, test_start_time, test_end_time, total_duration)


def generate_report(config: Dict, metrics_list: List[Dict], start_time: float, end_time: float, duration: float):
    """Generate detailed performance report."""
    # Create reports directory
    reports_dir = Path("var/mcp-bench/reports")
    reports_dir.mkdir(parents=True, exist_ok=True)

    # Generate timestamp for report file
    timestamp = datetime.fromtimestamp(start_time).strftime("%Y%m%d_%H%M%S")
    report_file = reports_dir / f"perf_report_{timestamp}.json"

    # Aggregate metrics
    total_requests = sum(m['total_requests'] for m in metrics_list)
    total_successful = sum(m['successful_requests'] for m in metrics_list)
    total_failed = sum(m['failed_requests'] for m in metrics_list)

    # Calculate aggregate response time
    all_response_times = []
    for metrics in metrics_list:
        if metrics['avg_response_time'] > 0:
            all_response_times.append(metrics['avg_response_time'])
    avg_response_time = sum(all_response_times) / len(all_response_times) if all_response_times else 0

    # Build detailed report
    report_data = {
        "timestamp": datetime.fromtimestamp(start_time).isoformat(),
        "test_duration": duration,
        "configuration": config,
        "summary": {
            "total_requests": total_requests,
            "successful_requests": total_successful,
            "failed_requests": total_failed,
            "success_rate": (total_successful / total_requests * 100) if total_requests > 0 else 0,
            "avg_response_time_ms": avg_response_time * 1000,
            "overall_throughput_rps": total_requests / duration if duration > 0 else 0
        },
        "streams": []
    }

    # Add per-stream details
    for metrics in metrics_list:
        stream_data = {
            "stream_id": metrics['stream_id'],
            "metrics": {
                "total_requests": metrics['total_requests'],
                "successful_requests": metrics['successful_requests'],
                "failed_requests": metrics['failed_requests'],
                "success_rate": metrics['success_rate'],
                "avg_response_time_ms": metrics['avg_response_time'] * 1000,
                "min_response_time_ms": metrics['min_response_time'] * 1000,
                "max_response_time_ms": metrics['max_response_time'] * 1000,
                "throughput_rps": metrics['requests_per_second'],
                "duration": metrics['duration']
            }
        }
        report_data["streams"].append(stream_data)

    # Save report to file
    with open(report_file, 'w') as f:
        json.dump(report_data, f, indent=2)

    print(f"ðŸ“Š Detailed report saved to: {report_file}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="MCP Performance Test Runner")
    parser.add_argument("config", help="Configuration file path")
    parser.add_argument("-v", "--verbose", action="store_true",
                        help="Show detailed request/response information")
    args = parser.parse_args()

    try:
        asyncio.run(run_test(args.config, args.verbose))
    except KeyboardInterrupt:
        print("\nTest interrupted")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)